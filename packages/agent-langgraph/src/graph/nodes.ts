/**
 * èŠ‚ç‚¹å®šä¹‰
 * LangGraph çŠ¶æ€æœºçš„å„ä¸ªèŠ‚ç‚¹
 */

import { HumanMessage, AIMessage, SystemMessage } from "@langchain/core/messages";
import { ChatAnthropic } from "@langchain/anthropic";
import { logger, QuickReplyOption } from "@civil-agent/core";
import type { UserIntent } from "@civil-agent/core";
import { SYSTEM_PROMPTS } from "../prompts/system-prompts";
import { TASK_PROMPTS } from "../prompts/task-prompts";
import { getMCPToolClient } from "../tools/mcp-tools";
import { TimeTools, StringTools, ProgressTools, LogTools } from "../tools/local-tools";
import { getEmotionDetector } from "../middleware/emotion-detector";
import { getContextEnhancer } from "../middleware/context-enhancer";
import { getAgentConfig } from "../config/agent.config";
import type { GraphStateType } from "./state";

/**
 * åˆ›å»ºå¿«æ·å›å¤é€‰é¡¹
 */
function createQuickReplies(texts: string[]): QuickReplyOption[] {
  return texts.map((text) => ({
    id: `qr-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`,
    text,
    action: text,
  }));
}

/**
 * æ„å›¾è¯†åˆ«èŠ‚ç‚¹
 */
export async function intentRecognitionNode(
  state: GraphStateType
): Promise<Partial<GraphStateType>> {
  logger.info("Intent recognition node executing");

  const lastMessage = state.messages[state.messages.length - 1];
  const content = lastMessage.content as string;

  const intentPrompt = SYSTEM_PROMPTS.INTENT_RECOGNITION.replace("{message}", content);

  try {
    const config = getAgentConfig();
    const llm = new ChatAnthropic({
      model: config.llm.model,
      temperature: 0,
      maxTokens: 100,
      apiKey: config.llm.apiKey,
    });

    const response = await llm.invoke([new HumanMessage(intentPrompt)]);
    const intentText = response.content as string;
    const intent = intentText.trim() as UserIntent;

    logger.info(`Detected intent: ${intent}`);

    return {
      userIntent: intent,
    };
  } catch (error) {
    const err = error instanceof Error ? error : new Error(String(error));
    logger.error("Intent recognition failed", err);
    return {
      userIntent: "general_inquiry",
    };
  }
}

/**
 * æ—©å®‰é—®å€™èŠ‚ç‚¹
 */
export async function morningGreetingNode(
  state: GraphStateType
): Promise<Partial<GraphStateType>> {
  logger.info("Morning greeting node executing");

  const config = getAgentConfig();
  const llm = new ChatAnthropic({
    model: config.llm.model,
    temperature: config.llm.temperature,
    maxTokens: config.llm.maxTokens,
    apiKey: config.llm.apiKey,
  });

  try {
    const contextEnhancer = getContextEnhancer();
    const context = await contextEnhancer.enhanceContext(state.userId, "");

    const systemPrompt = SYSTEM_PROMPTS.MORNING_GREETING;
    const userPrompt = `ç”¨æˆ·IDï¼š${state.userId}\n${contextEnhancer.generateSystemPromptEnhancement(context)}`;

    const response = await llm.invoke([
      new SystemMessage(systemPrompt),
      new HumanMessage(userPrompt),
    ]);

    const quickReplies = ["å¼€å§‹ä»Šå¤©çš„å­¦ä¹ ", "è°ƒæ•´å­¦ä¹ è®¡åˆ’", "æŸ¥çœ‹å­¦ä¹ è¿›åº¦"];

    LogTools.logAgentDecision(state.userId, state.userIntent, "Morning greeting");

    return {
      messages: [...state.messages, new AIMessage(response.content as string)],
      quickReplyOptions: createQuickReplies(quickReplies),
      waitingForUserInput: true,
    };
  } catch (error) {
    const err = error instanceof Error ? error : new Error(String(error));
    logger.error("Morning greeting failed", err);
    const errorMessage = "æ—©ä¸Šå¥½ï¼â˜€ï¸ ä»Šå¤©åˆæ˜¯å……æ»¡å¸Œæœ›çš„ä¸€å¤©ã€‚å‡†å¤‡å¥½äº†å—ï¼Ÿ";
    return {
      messages: [...state.messages, new AIMessage(errorMessage)],
      quickReplyOptions: createQuickReplies(["å¼€å§‹ä»Šå¤©çš„å­¦ä¹ ", "è°ƒæ•´å­¦ä¹ è®¡åˆ’", "æŸ¥çœ‹å­¦ä¹ è¿›åº¦"]),
      waitingForUserInput: true,
    };
  }
}

/**
 * æ™šé—´å¤ç›˜èŠ‚ç‚¹
 */
export async function eveningReviewNode(
  state: GraphStateType
): Promise<Partial<GraphStateType>> {
  logger.info("Evening review node executing");

  const config = getAgentConfig();
  const llm = new ChatAnthropic({
    model: config.llm.model,
    temperature: config.llm.temperature,
    maxTokens: config.llm.maxTokens,
    apiKey: config.llm.apiKey,
  });

  try {
    const contextEnhancer = getContextEnhancer();
    const context = await contextEnhancer.enhanceContext(state.userId, "");

    const systemPrompt = SYSTEM_PROMPTS.EVENING_REVIEW;
    const userPrompt = `ç”¨æˆ·IDï¼š${state.userId}\n${contextEnhancer.generateSystemPromptEnhancement(context)}`;

    const response = await llm.invoke([
      new SystemMessage(systemPrompt),
      new HumanMessage(userPrompt),
    ]);

    const quickReplies = ["è®°å½•ä»Šå¤©çš„å­¦ä¹ å¿ƒå¾—", "æŸ¥çœ‹æœ¬å‘¨æ•°æ®", "å‡†å¤‡ä¼‘æ¯"];

    LogTools.logAgentDecision(state.userId, state.userIntent, "Evening review");

    return {
      messages: [...state.messages, new AIMessage(response.content as string)],
      quickReplyOptions: createQuickReplies(quickReplies),
      waitingForUserInput: true,
    };
  } catch (error) {
    const err = error instanceof Error ? error : new Error(String(error));
    logger.error("Evening review failed", err);
    const errorMessage = "æ™šä¸Šå¥½ï¼ğŸŒ™ ä»Šå¤©è¾›è‹¦äº†ï¼Œæ—©ç‚¹ä¼‘æ¯å“¦ï½";
    return {
      messages: [...state.messages, new AIMessage(errorMessage)],
      quickReplyOptions: createQuickReplies(["è®°å½•ä»Šå¤©çš„å­¦ä¹ å¿ƒå¾—", "æŸ¥çœ‹æœ¬å‘¨æ•°æ®", "å‡†å¤‡ä¼‘æ¯"]),
      waitingForUserInput: true,
    };
  }
}

/**
 * ä»»åŠ¡ç”ŸæˆèŠ‚ç‚¹
 */
export async function taskGenerationNode(
  state: GraphStateType
): Promise<Partial<GraphStateType>> {
  logger.info("Task generation node executing");

  const config = getAgentConfig();
  const llm = new ChatAnthropic({
    model: config.llm.model,
    temperature: config.llm.temperature,
    maxTokens: config.llm.maxTokens,
    apiKey: config.llm.apiKey,
  });

  try {
    const mcpClient = getMCPToolClient();
    const ragResult = await mcpClient.searchKnowledge({
      query: `ç”¨æˆ· ${state.userId} çš„å­¦ä¹ è¿›åº¦å’Œè–„å¼±æ¨¡å—`,
      category: "user_history",
      topK: 3,
    });

    let ragContext = "";
    if (ragResult.success && ragResult.data?.results?.length > 0) {
      ragContext = ragResult.data.results.map((r: any) => r.content).join("\n");
    }

    const systemPrompt = SYSTEM_PROMPTS.TASK_GENERATION;
    const userPrompt = TASK_PROMPTS.GENERATE_TASK_PLAN
      .replace("{userId}", state.userId)
      .replace("{progress}", ragContext || "æš‚æ— è¿›åº¦æ•°æ®")
      .replace("{weakModules}", "å¾…åˆ†æ")
      .replace("{studyHabits}", "å¾…åˆ†æ");

    const response = await llm.invoke([
      new SystemMessage(systemPrompt),
      new HumanMessage(userPrompt),
    ]);

    const quickReplies = ["ç¡®è®¤è®¡åˆ’", "è°ƒæ•´ä»»åŠ¡", "å–æ¶ˆ"];

    LogTools.logAgentDecision(state.userId, state.userIntent, "Task generation");

    return {
      messages: [...state.messages, new AIMessage(response.content as string)],
      quickReplyOptions: createQuickReplies(quickReplies),
      waitingForUserInput: true,
      ragResults: ragResult.success ? ragResult.data?.results : [],
    };
  } catch (error) {
    const err = error instanceof Error ? error : new Error(String(error));
    logger.error("Task generation failed", err);
    const errorMessage = "æŠ±æ­‰ï¼Œç”Ÿæˆä»»åŠ¡è®¡åˆ’æ—¶å‡ºé”™äº†ã€‚è¯·ç¨åå†è¯•ã€‚";
    return {
      messages: [...state.messages, new AIMessage(errorMessage)],
      quickReplyOptions: createQuickReplies(["é‡è¯•", "å–æ¶ˆ"]),
      waitingForUserInput: true,
    };
  }
}

/**
 * æƒ…æ„Ÿæ”¯æŒèŠ‚ç‚¹
 */
export async function emotionSupportNode(
  state: GraphStateType
): Promise<Partial<GraphStateType>> {
  logger.info("Emotion support node executing");

  const config = getAgentConfig();
  const llm = new ChatAnthropic({
    model: config.llm.model,
    temperature: config.llm.temperature,
    maxTokens: config.llm.maxTokens,
    apiKey: config.llm.apiKey,
  });

  try {
    const emotionDetector = getEmotionDetector();
    const lastMessage = state.messages[state.messages.length - 1];
    const content = lastMessage.content as string;

    const emotionResult = emotionDetector.detectEmotion(content);
    const emotionLabel = emotionDetector.getEmotionLabel(emotionResult.emotion);
    const emotionDescription = emotionDetector.getEmotionDescription(
      emotionResult.emotion,
      emotionResult.intensity
    );

    const mcpClient = getMCPToolClient();
    const ragResult = await mcpClient.searchKnowledge({
      query: `${emotionResult.emotion} å¤‡è€ƒç»éªŒ è§£å†³æ–¹æ¡ˆ`,
      category: "exam_experience",
      topK: 3,
    });

    let ragContext = "";
    if (ragResult.success && ragResult.data?.results?.length > 0) {
      ragContext = ragResult.data.results.map((r: any) => r.content).join("\n");
    }

    const systemPrompt = SYSTEM_PROMPTS.EMOTION_SUPPORT;
    const userPrompt = `ç”¨æˆ·æƒ…ç»ªï¼š${emotionLabel} (${emotionDescription})\nç›¸å…³ç»éªŒï¼š${ragContext || "æš‚æ— ç›¸å…³ç»éªŒ"}\nç”¨æˆ·æ¶ˆæ¯ï¼š${content}`;

    const response = await llm.invoke([
      new SystemMessage(systemPrompt),
      new HumanMessage(userPrompt),
    ]);

    const quickReplies = ["åˆ†æè–„å¼±æ¨¡å—", "åˆ¶å®šçªç ´è®¡åˆ’", "æˆ‘å†æƒ³æƒ³"];

    LogTools.logAgentDecision(state.userId, state.userIntent, "Emotion support");

    return {
      messages: [...state.messages, new AIMessage(response.content as string)],
      quickReplyOptions: createQuickReplies(quickReplies),
      waitingForUserInput: true,
      ragResults: ragResult.success ? ragResult.data?.results : [],
    };
  } catch (error) {
    const err = error instanceof Error ? error : new Error(String(error));
    logger.error("Emotion support failed", err);
    const errorMessage = "æˆ‘ç†è§£ä½ çš„æ„Ÿå—ï¼Œæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ";
    return {
      messages: [...state.messages, new AIMessage(errorMessage)],
      quickReplyOptions: createQuickReplies(["ç»§ç»­å¯¹è¯", "ç»“æŸå¯¹è¯"]),
      waitingForUserInput: true,
    };
  }
}

/**
 * è¿›åº¦æŸ¥è¯¢èŠ‚ç‚¹
 */
export async function progressQueryNode(
  state: GraphStateType
): Promise<Partial<GraphStateType>> {
  logger.info("Progress query node executing");

  const config = getAgentConfig();
  const llm = new ChatAnthropic({
    model: config.llm.model,
    temperature: config.llm.temperature,
    maxTokens: config.llm.maxTokens,
    apiKey: config.llm.apiKey,
  });

  try {
    const mcpClient = getMCPToolClient();
    const ragResult = await mcpClient.searchKnowledge({
      query: `ç”¨æˆ· ${state.userId} çš„å­¦ä¹ è¿›åº¦æ•°æ®`,
      category: "user_history",
      topK: 5,
    });

    let progressData = "";
    if (ragResult.success && ragResult.data?.results?.length > 0) {
      progressData = ragResult.data.results.map((r: any) => r.content).join("\n");
    }

    const systemPrompt = SYSTEM_PROMPTS.GENERAL_QA;
    const userPrompt = TASK_PROMPTS.QUERY_PROGRESS.replace("{userId}", state.userId);

    const response = await llm.invoke([
      new SystemMessage(systemPrompt),
      new HumanMessage(userPrompt),
    ]);

    LogTools.logAgentDecision(state.userId, state.userIntent, "Progress query");

    return {
      messages: [...state.messages, new AIMessage(response.content as string)],
      quickReplyOptions: [],
      waitingForUserInput: false,
      ragResults: ragResult.success ? ragResult.data?.results : [],
    };
  } catch (error) {
    const err = error instanceof Error ? error : new Error(String(error));
    logger.error("Progress query failed", err);
    const errorMessage = "æŠ±æ­‰ï¼ŒæŸ¥è¯¢å­¦ä¹ è¿›åº¦æ—¶å‡ºé”™äº†ã€‚è¯·ç¨åå†è¯•ã€‚";
    return {
      messages: [...state.messages, new AIMessage(errorMessage)],
      quickReplyOptions: [],
      waitingForUserInput: false,
    };
  }
}

/**
 * ä¸€èˆ¬é—®ç­”èŠ‚ç‚¹
 */
export async function generalQANode(
  state: GraphStateType
): Promise<Partial<GraphStateType>> {
  logger.info("General QA node executing");

  const config = getAgentConfig();
  const llm = new ChatAnthropic({
    model: config.llm.model,
    temperature: config.llm.temperature,
    maxTokens: config.llm.maxTokens,
    apiKey: config.llm.apiKey,
  });

  try {
    const lastMessage = state.messages[state.messages.length - 1];
    const content = lastMessage.content as string;

    const contextEnhancer = getContextEnhancer();
    const enhancedMessage = await contextEnhancer.enhanceUserMessage(state.userId, content);

    const systemPrompt = SYSTEM_PROMPTS.DEFAULT;
    const response = await llm.invoke([
      new SystemMessage(systemPrompt),
      new HumanMessage(enhancedMessage),
    ]);

    LogTools.logAgentDecision(state.userId, state.userIntent, "General QA");

    return {
      messages: [...state.messages, new AIMessage(response.content as string)],
      quickReplyOptions: [],
      waitingForUserInput: false,
    };
  } catch (error) {
    const err = error instanceof Error ? error : new Error(String(error));
    logger.error("General QA failed", err);
    const errorMessage = "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç†è§£ä½ çš„é—®é¢˜ã€‚è¯·æ¢ä¸ªæ–¹å¼é—®æˆ‘ã€‚";
    return {
      messages: [...state.messages, new AIMessage(errorMessage)],
      quickReplyOptions: [],
      waitingForUserInput: false,
    };
  }
}

/**
 * å“åº”ç”ŸæˆèŠ‚ç‚¹
 */
export async function generateResponseNode(
  state: GraphStateType
): Promise<Partial<GraphStateType>> {
  logger.info("Generate response node executing");

  return {
    waitingForUserInput: false,
  };
}